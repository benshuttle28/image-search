# image-search
Design Overview: For my final project, I chose to create a website that allows a user to use an image to generate a search query. Simply called Image search, the website takes an uploaded image and uses the Google Vertex AI API to return a description of the uploaded image. The description is generated after the Upload and Process button is clicked. This description is then automatically placed in an embedded search bar on the webpage where the user can then click submit to see the results of the search query. The Google Custom Search API is leveraged to perform the Google search. 
 
My final design is quite similar to my original proposal. Exceptions to this are that I was originally planning to perform the image processing and labeling locally by developing my own computer vision algorithm. This idea was quite ambitious given the short time frame, and for that reason I chose to use an API. One other modification from the original design was using the Custom Search API in place of the Google Programmable Search Engine (PSE). The PSE is a nice resource to easily embed a Google search bar in a webpage, but I opted to not use this as it was difficult to automatically load the image description into the search bar. 

I feel that this project was justified for the given time frame of 6 weeks. Building a website is not something I have done before and creating it from the ground up using HTML will take some time. Also, I will need to review the documentation for the Google Cloud API. For those reasons, I feel that the project is not too easy, and considering the vast number of resources surrounding web development and computer vision, I feel that this project is not over-ambitious.

 
Preliminary Design Verification: The first step I took in verifying the design was to test the Vertex AI API call. I did this by running the backend python file independently and putting a test image into the directory that the code would look for to process. Once the API call was complete, the return image description was printed to the terminal.
 
The next step in testing was to test the web interface. I did this by running the web_ui.html file independently and viewing the HTML page in a web browser. The webpage showed all headers and text in the correct location. After verifying both the frontend and backend code, I determined that my design was certainly feasible. 

Design Implementation: In order to implement my design, I created three different subcomponents. The first was the backend Python file that contains a function to take in the uploaded image, send it to the API, and return the description of the image. 
The second is subcomponent is a frontend UI file that contains all of the HTML code for the website, as well as JavaScript functions that perform actions like filling the search box with the description automatically, performing the search query API call, and displaying the results of the search query. 
The final subcomponent is the top-level Python file app file. This file uses the Flask framework integrate my backend Python code with my frontend HTML code to make a standalone web application. This file calls the function to send the uploaded image to the Vertex AI API and also passes the description of the image to the HTML file. When this file is run, it creates an instance of the web application that is locally hosted. 
My design process was the same as creating any other application. I wanted to have separate code for the backend and the frontend, and I knew that I would need a way to integrate them together. I found Flask as an easy way to integrate Python web apps and used that as the top-level Python file to run the app. Flask is an easy framework to use with good documentation, and I did not experience any challenges.

Design Testing: In order to test my design, I utilized the console in the inspector on Firefox to be able to print any relevant information and see any errors or exceptions that were occurring. 
 
Another helpful tool in testing the website frontend specifically was the style editor, which allowed me to change the font and colors of the interface in real-time to see what looked best. One challenge that I encountered while testing was having the generated description of the image populate in the search bar. When I was initially testing using the PSE, I could not get the description to load into the search bar. By using a print statement to the console log, I was able to see that the description was successfully being returned and placed into the variable I created to hold the search query. The text was just simply not being preloaded into the search bar, which is why I went a different route in using a basic embedded search bar with an API call to the Google Custom Search API. Video of functionality

Stay tuned for the link to access the website on your own device!

